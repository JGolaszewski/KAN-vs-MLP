{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0786e188",
   "metadata": {},
   "source": [
    "# MLP breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8b5996",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14443f6",
   "metadata": {},
   "source": [
    "#### What is a Multilayer Perceptron (MLP)?\n",
    "\n",
    "A **Multilayer Perceptron (MLP)** is a class of **feedforward artificial neural network** composed of multiple layers of interconnected neurons. It maps input vectors $\\mathbf{x} \\in \\mathbb{R}^n$ to output predictions $\\hat{\\mathbf{y}} \\in \\mathbb{R}^m$ through a sequence of **learned linear transformations** followed by **nonlinear activations**.\n",
    "\n",
    "An MLP typically consists of:\n",
    "\n",
    "1. **Input Layer**:  \n",
    "    Receives the input features. No computations are performed here‚Äîthis layer simply passes the data to the first hidden layer.\n",
    "    \n",
    "2. **One or More Hidden Layers**:  \n",
    "    Each hidden layer computes a transformation of the form:\n",
    "    \n",
    "    $\\mathbf{a}^{(l)}=\\sigma\\left(\\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)}\\right)$\n",
    "    \n",
    "    where:\n",
    "    \n",
    "    - $\\mathbf{W}^{(l)}$ and $\\mathbf{b}^{(l)}$ are the learnable weight matrix and bias vector\n",
    "        \n",
    "    - $\\sigma(\\cdot)$ is a nonlinear activation function (e.g., sigmoid, ReLU)\n",
    "        \n",
    "    - $\\mathbf{a}^{(l-1)}$ is the activation from the previous layer\n",
    "        \n",
    "3. **Output Layer**:  \n",
    "    Computes the final prediction. The activation here may depend on the task (e.g., softmax for classification, linear for regression).\n",
    "\n",
    "#### Learning\n",
    "\n",
    "The MLP is trained to minimize a **loss function** $\\mathcal{L}(\\hat{\\mathbf{y}}, \\mathbf{y})$ over the training set by adjusting weights and biases using **backpropagation** combined with **gradient descent**. Gradients of the loss are propagated backwards through the network using the chain rule.\n",
    "\n",
    "#### Credits\n",
    "\n",
    "This Multilayer Perceptron (MLP) implementation is inspired by and based on **Omar Aflak‚Äôs** excellent work on building a neural network from scratch in Python. His clear, step-by-step Medium article and accompanying GitHub repository laid the foundation for this simple implementation:\n",
    "\n",
    "- üìñ [Medium Article: ‚ÄúNeural Network From Scratch in Python‚Äù](https://medium.com/data-science/math-neural-network-from-scratch-in-python-d6da9f29ce65)  \n",
    "- üíª [GitHub Repo: OmarAflak/Medium-Python-Neural-Network](https://github.com/OmarAflak/Medium-Python-Neural-Network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a52d89",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdafb700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed04908",
   "metadata": {},
   "source": [
    "## Simple Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6e677",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d625b40",
   "metadata": {},
   "source": [
    "For the purpose of this notebook, we are going to use the **sigmoid function** as our activation function.\n",
    "\n",
    "The sigmoid is a smooth, S-shaped function that maps any real-valued number into the range (0, 1).\n",
    "\n",
    "The formula for the sigmoid function is:\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "Its derivative, which is needed during backpropagation, is:\n",
    "\n",
    "$\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b78786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cb11c2",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d0b85",
   "metadata": {},
   "source": [
    "To measure how well our neural network is performing, we will use the **Mean Squared Error (MSE)** loss function.\n",
    "\n",
    "The MSE is a standard loss function for regression problems. It computes the average of the squared differences between the predicted and the actual values:\n",
    "\n",
    "$\\text{MSE}(y, \\hat{y}) = \\frac{1}{2n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "We include the factor of 1/2 for mathematical convenience when taking derivatives during backpropagation. The gradient (derivative of the loss with respect to the prediction) is:\n",
    "\n",
    "$\\frac{\\partial \\text{MSE}}{\\partial \\hat{y}} = \\hat{y} - y$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b3194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return (0.5*(y_true - y_pred)**2).mean()\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return y_pred - y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3ffdc",
   "metadata": {},
   "source": [
    "### Activation Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b39925",
   "metadata": {},
   "source": [
    "The **ActivationLayer** applies a non-linear activation function element-wise to its input. In our case, we use the **sigmoid** activation function defined above.\n",
    "\n",
    "This operation introduces non-linearity into the network, enabling it to model complex, non-linear decision boundaries. Without such functions, any composition of layers would reduce to a linear map.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è© Forward Pass:\n",
    "\n",
    "For a given input vector $\\mathbf{z} \\in \\mathbb{R}^{1 \\times m}$, the activation is applied element-wise:\n",
    "\n",
    "$\\mathbf{a} = \\sigma(\\mathbf{z})$\n",
    "\n",
    "where $\\mathbf{a} \\in \\mathbb{R}^{1 \\times m}$ is the output activation vector.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è™ Backward Pass:\n",
    "\n",
    "Given the upstream gradient from the next layer $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}}$, we compute the local gradient of the activation:\n",
    "\n",
    "$\\sigma'(z_i) = \\sigma(z_i)(1 - \\sigma(z_i)) \\quad \\text{for each component } z_i \\in \\mathbf{z}$\n",
    "\n",
    "The resulting gradient with respect to the input $\\mathbf{z}$ is:\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}} \\circ \\sigma'(\\mathbf{z})$\n",
    "\n",
    "Here, $\\circ$ denotes the Hadamard product. This propagates gradients through the activation nonlinearity in the backward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8eb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def forward(self, input_data):\n",
    "        self.input = input_data\n",
    "        return sigmoid(input_data)\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        return sigmoid_prime(self.input) * output_error\n",
    "    \n",
    "    def step(self, eta):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360802d",
   "metadata": {},
   "source": [
    "### Fully Connected Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b039322e",
   "metadata": {},
   "source": [
    "The **FullyConnectedLayer** (dense layer) implements an affine transformation:\n",
    "\n",
    "$\\mathbf{z} = \\mathbf{x} \\cdot \\mathbf{W} + \\mathbf{b}$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbf{x} \\in \\mathbb{R}^{1 \\times n}$: input row vector\n",
    "    \n",
    "- $\\mathbf{W} \\in \\mathbb{R}^{n \\times m}$: weight matrix\n",
    "    \n",
    "- $\\mathbf{b} \\in \\mathbb{R}^{1 \\times m}$: bias vector\n",
    "    \n",
    "- $\\mathbf{z} \\in \\mathbb{R}^{1 \\times m}$: output of the layer (before activation)\n",
    "    \n",
    "\n",
    "This operation projects the input from an $N$-dimensional space to an $M$-dimensional space.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è© Forward Pass:\n",
    "\n",
    "Given input vector $\\mathbf{x}$, the layer computes:\n",
    "\n",
    "$\\mathbf{z} = \\mathbf{x} \\cdot \\mathbf{W} + \\mathbf{b}$\n",
    "\n",
    "This linear output vector $\\mathbf{z}$ is then passed to the next layer followed by activation layer.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è™ Backward Pass:\n",
    "\n",
    "Assuming we are given an upstream gradient $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}}$ from the loss $\\mathcal{L}$, we compute:\n",
    "\n",
    "- **Input Gradient** (to propagate to previous layer):\n",
    "    \n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}} \\cdot \\mathbf{W}^\\top$\n",
    "\n",
    "- **Weights Gradient** (for update step):\n",
    "    \n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}} = \\mathbf{x}^\\top \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}}$\n",
    "\n",
    "- **Bias Gradient**:\n",
    "    \n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}}$\n",
    "\n",
    "These partial derivatives are accumulated across all samples in the mini-batch. The `step()` method then applies the parameter update via averaged gradients:\n",
    "\n",
    "$\\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\cdot \\frac{1}{B} \\sum_{i=1}^B \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\mathbf{W}}, \\quad \\mathbf{b} \\leftarrow \\mathbf{b} - \\eta \\cdot \\frac{1}{B} \\sum_{i=1}^B \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\mathbf{b}}$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\eta$: learning rate\n",
    "    \n",
    "- $B$: batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd27933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.delta_w = np.zeros((input_size, output_size))\n",
    "        self.delta_b = np.zeros((1,output_size))\n",
    "        self.passes = 0\n",
    "\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input = input_data\n",
    "        return np.dot(self.input, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "\n",
    "        self.delta_w += weights_error\n",
    "        self.delta_b += output_error\n",
    "        self.passes += 1\n",
    "        return input_error\n",
    "\n",
    "    def step(self, eta):\n",
    "        self.weights -= eta * self.delta_w / self.passes\n",
    "        self.bias -= eta * self.delta_b / self.passes\n",
    "\n",
    "        self.delta_w = np.zeros(self.weights.shape)\n",
    "        self.delta_b = np.zeros(self.bias.shape)\n",
    "        self.passes = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a374642",
   "metadata": {},
   "source": [
    "### MLP Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c123d",
   "metadata": {},
   "source": [
    "The `Network` class encapsulates the full structure and training loop of a feedforward neural network. It manages the sequence of layers, orchestrates forward and backward propagation, and handles parameter updates via mini-batch gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÆ Predictions\n",
    "\n",
    "Predictions are generated using the `predict` method, which computes the network output by successively applying the forward operation of each layer.\n",
    "Given an input sample $\\mathbf{x}$, the output is computed as:\n",
    "\n",
    "$\\hat{\\mathbf{y}} = L_n \\circ L_{n-1} \\circ \\dots \\circ L_1(\\mathbf{x}) \\equiv \\mathbf{a}^{(n)}$\n",
    "\n",
    "where $\\mathbf{a}^{(n)}$ denotes the activation of the final layer.\n",
    "\n",
    "---\n",
    "\n",
    "#### üß† Training\n",
    "\n",
    "Training of network is handled by `fit` method, which computes network predictions by forwarding mini-batch of $B$ samples randomly selected from empirical dataset.\n",
    "\n",
    "$\\mathcal{B} = \\text{RandomBatch}(\\mathcal{D}, B) = \\{ (\\mathbf{x}^{(i)}, \\mathbf{y}^{(i)}) \\}_{i=1}^B$\n",
    "\n",
    "For each sample $\\mathbf{x}^{(i)}$, a **forward pass** is performed through the network to compute the prediction:\n",
    "\n",
    "$\\hat{\\mathbf{y}}^{(i)} = f(\\mathbf{x}^{(i)}; \\theta)$\n",
    "\n",
    "where $\\theta$ represents the collection of all trainable parameters.\n",
    "\n",
    "The output is compared with the label $\\mathbf{y}^{(i)}$, and the **loss** for the sample is computed using the mean squared error (MSE) function. Once the forward pass is complete, a **backward pass** propagates the error gradients back through the network. For each layer, we compute the partial derivatives: $\\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\theta}$, which measure how the loss changes with respect to the layer‚Äôs parameters. These gradients are accumulated over the entire mini-batch. \n",
    "\n",
    "After all $\\mathcal{B}$ samples are processed, the parameters are updated by a gradient descent step using the average of the accumulated gradients:\n",
    "\n",
    "$\\theta \\leftarrow \\theta - \\eta \\cdot \\frac{1}{B} \\sum_{i=1}^B \\nabla_\\theta \\mathcal{L}^{(i)}$\n",
    "\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "Those steps are later repeted for specified number of epoches $E$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b85481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, verbose=True):\n",
    "        self.verbose = verbose\n",
    "        self.layers = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        result = []\n",
    "        for i in range(input_data.shape[0]):\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward(output)\n",
    "            result.append(output)\n",
    "        return result\n",
    "\n",
    "    def fit(self, x_train, y_train, epoches, learning_rate, batch_size=64):\n",
    "        for i in range(epoches):\n",
    "            err = 0\n",
    "\n",
    "            idx = np.argsort(np.random.random(x_train.shape[0]))[:batch_size]\n",
    "            x_batch = x_train[idx]\n",
    "            y_batch = y_train[idx]\n",
    "\n",
    "            for j in range(batch_size):\n",
    "                output = x_batch[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward(output)\n",
    "\n",
    "                err += mse(y_batch[j], output)\n",
    "\n",
    "                error = mse_prime(y_batch[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward(error)\n",
    "            \n",
    "            for layer in self.layers:\n",
    "                layer.step(learning_rate)\n",
    "\n",
    "            if (self.verbose) and ((i%10) == 0):\n",
    "                err /= batch_size\n",
    "                print('epoch: %5d/%d   error=%0.9f' % (i, epoches, err))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
