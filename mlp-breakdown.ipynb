{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0786e188",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14443f6",
   "metadata": {},
   "source": [
    "**üéâ Implementation Credit & Introduction üöÄ**\n",
    "\n",
    "This Multilayer Perceptron (MLP) implementation is inspired by and based on Omar Aflak‚Äôs excellent work on building a neural network from scratch in Python. His clear, step-by-step Medium article and accompanying GitHub repository laid the foundation for this notebook:\n",
    "\n",
    "- üìñ [Medium Article: ‚ÄúMath & Neural Network From Scratch in Python‚Äù](https://medium.com/data-science/math-neural-network-from-scratch-in-python-d6da9f29ce65)  \n",
    "- üíª [GitHub Repo: OmarAflak/Medium-Python-Neural-Network](https://github.com/OmarAflak/Medium-Python-Neural-Network)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ú® What Is a Multilayer Perceptron?\n",
    "\n",
    "A Multilayer Perceptron is one of the simplest‚Äîand yet most powerful‚Äîtypes of feedforward neural networks. It consists of:\n",
    "\n",
    "1. **Input Layer**: Receives the raw features (e.g., pixel values, sensor readings).  \n",
    "2. **Hidden Layer(s)**: Perform non-linear transformations using activation functions like sigmoid, ReLU, or tanh.  \n",
    "3. **Output Layer**: Produces the final predictions (e.g., class probabilities, regression outputs).\n",
    "\n",
    "Each layer is ‚Äúfully connected,‚Äù meaning every neuron in one layer connects to every neuron in the next. The magic of MLPs lies in how they learn: through **backpropagation**, weights and biases are adjusted to minimize a loss function (e.g., mean squared error).\n",
    "\n",
    "---\n",
    "\n",
    "## üé® Visual Overview\n",
    "\n",
    "Below are some placeholders where you can insert illustrative diagrams:\n",
    "\n",
    "![üñºÔ∏è Network Architecture Placeholder](path/to/architecture_diagram.png)  \n",
    "*Figure 1: Schematic of an MLP with one hidden layer.*\n",
    "\n",
    "![üñºÔ∏è Forward & Backward Pass Placeholder](path/to/backprop_diagram.png)  \n",
    "*Figure 2: Illustration of forward propagation (left) and backpropagation (right).*\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Notebook Structure\n",
    "\n",
    "1. **Import Libraries**  \n",
    "2. **Activation & Loss Functions**  \n",
    "3. **Layer Classes**  \n",
    "   - Fully Connected Layer  \n",
    "   - Activation Layer  \n",
    "4. **Network Class & Training Loop**  \n",
    "5. **Example: Training on Dummy Data**  \n",
    "6. **Results & Visualization**\n",
    "\n",
    "Let‚Äôs dive in! ‚ú®  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a52d89",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdafb700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6e677",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b78786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cb11c2",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b3194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return (0.5*(y_true - y_pred)**2).mean()\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return y_pred - y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3ffdc",
   "metadata": {},
   "source": [
    "## Activation Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8eb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def forward(self, input_data):\n",
    "        self.input = input_data\n",
    "        return sigmoid(input_data)\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        return sigmoid_prime(self.input) * output_error\n",
    "    \n",
    "    def step(self, eta):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360802d",
   "metadata": {},
   "source": [
    "## Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd27933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.delta_w = np.zeros((input_size, output_size))\n",
    "        self.delta_b = np.zeros((1,output_size))\n",
    "        self.passes = 0\n",
    "\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input = input_data\n",
    "        return np.dot(self.input, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "\n",
    "        self.delta_w += weights_error\n",
    "        self.delta_b += output_error\n",
    "        self.passes += 1\n",
    "        return input_error\n",
    "\n",
    "    def step(self, eta):\n",
    "        self.weights -= eta * self.delta_w / self.passes\n",
    "        self.bias -= eta * self.delta_b / self.passes\n",
    "\n",
    "        self.delta_w = np.zeros(self.weights.shape)\n",
    "        self.delta_b = np.zeros(self.bias.shape)\n",
    "        self.passes = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a374642",
   "metadata": {},
   "source": [
    "## MLP Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b85481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, verbose=True):\n",
    "        self.verbose = verbose\n",
    "        self.layers = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        result = []\n",
    "        for i in range(input_data.shape[0]):\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward(output)\n",
    "            result.append(output)\n",
    "        return result\n",
    "\n",
    "    def fit(self, x_train, y_train, minibatches, learning_rate, batch_size=64):\n",
    "        for i in range(minibatches):\n",
    "            err = 0\n",
    "\n",
    "            idx = np.argsort(np.random.random(x_train.shape[0]))[:batch_size]\n",
    "            x_batch = x_train[idx]\n",
    "            y_batch = y_train[idx]\n",
    "\n",
    "            for j in range(batch_size):\n",
    "                output = x_batch[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward(output)\n",
    "\n",
    "                err += mse(y_batch[j], output)\n",
    "\n",
    "                error = mse_prime(y_batch[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward(error)\n",
    "            \n",
    "            for layer in self.layers:\n",
    "                layer.step(learning_rate)\n",
    "\n",
    "            if (self.verbose) and ((i%10) == 0):\n",
    "                err /= batch_size\n",
    "                print('minipartia: %5d/%d   b≈ÇƒÖd=%0.9f' % (i, minibatches, err))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
